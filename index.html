<!DOCTYPE html>
<html>
  <head>
    <title>Zheng Gu</title>
    <meta author="Zheng Gu" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <style type="text/css">
      * {
        font-family: "Lato", Verdana, Helvetica, sans-serif;
      }

      a {
        color: #1772d0;
        text-decoration: none;
      }

      a:focus,
      a:hover {
        color: #f09228;
        text-decoration: none;
      }

      body {
        background-color: #f7f7f7;
      }

      td,
      th,
      tr,
      p,
      a {
        font-size: 14px;
      }

      table {
        border: 0;
      }

      heading {
        font-size: 22px;
      }

      papertitle {
        font-size: 14px;
        font-weight: 700;
      }

      name {
        font-size: 30px;
      }

      position {
        font-size: 18px;
      }

      span.highlight {
        background-color: #ffffd0;
      }

      .superscript {
        vertical-align: super;
        font-size: smaller;
      }

      .pintop {
        font-weight: bold;
        font-size: 15px;
        color: #d32a13;
      }

      .teaser {
        width: 100%;
        height: 100%;
        max-width: 100%;
        max-height: 100%;
        box-shadow: 0px 0px 5px #888888;
        border-radius: 3px;
      }
    </style>
    <link rel="shortcut icon" href="images/ICON.JPG" type="image/x-icon" />
    <link
      href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic"
      rel="stylesheet"
      type="text/css"
    />
  </head>

  <body>
    <table width="800" align="center" cellspacing="0" cellpadding="0">
      <tr>
        <td>
          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td width="67%" valign="middle">
                <p align="center">
                  <name>Zheng Gu &nbsp 顾峥</name>
                  <br />
                  <position>Assistant Professor</position> <br />
                </p>
                <p style="text-align: justify">
                  I am currently an Assistant Professor with the
                  <a href="https://vcc.tech/index"
                    >Visual Computing Research Center (VCC)</a
                  >
                  (led by
                  <a href="https://vcc.tech/~huihuang/home">Prof. Hui Huang</a>)
                  at the
                  <a href="https://csse.szu.edu.cn/"
                    >College of Computer Science and Software Engineering</a
                  >, <a href="https://www.szu.edu.cn/">Shenzhen University</a>.
                </p>
                <p style="text-align: justify">
                  I received my Ph.D. degree from the
                  <a href="https://cs.nju.edu.cn/rl/"
                    >Reasoning and Learning (RL) Group</a
                  >
                  at College of Computer, Nanjing University in 2024, advised by
                  <a href="https://cs.nju.edu.cn/gaoyang/">Prof. Yang Gao</a>
                  and
                  <a href="https://cs.nju.edu.cn/huojing">Prof. Jing Huo</a>. I
                  received my dual Ph.D. degree from Department of Computer
                  Science, City Univercity of Hong Kong, advised by
                  <a href="https://www.cityu.edu.hk/stfprofile/jingliao.htm"
                    >Prof. Jing Liao</a
                  >. Before that, I received my B.Sc. degree from Nanjing
                  University in 2017.
                </p>
                <p style="text-align: justify">
                  My research focuses on machine learning, pattern analysis, and
                  computer vision, aiming to enhance the transferability and
                  controllability of visual generative models, exploring their
                  applications in dynamic open-world scenarios such as few-shot
                  learning, in-context learning and continual learning.
                </p>
                <p>
                  Address: L6-811, Shenzhen University Cang Hai Campus,
                  Shenzhen, China
                </p>
                <p>Email: guzheng@szu.edu.cn</p>
                <p align="center">
                  <a
                    href="https://scholar.google.com/citations?user=Nwg_u4EAAAAJ"
                    >Google Scholar</a
                  >
                  &nbsp/&nbsp
                  <a href="https://github.com/edward3862">GitHub</a> &nbsp/&nbsp
                  <a href="https://csse.szu.edu.cn/pages/user/index?id=1330"
                    >中文</a
                  >
                </p>
              </td>
              <td style="padding: 2.5%; width: 40%; max-width: 40%">
                <img
                  style="
                    width: 100%;
                    max-width: 100%;
                    object-fit: cover;
                    border-radius: 50%;
                  "
                  src="images/guzheng.JPEG"
                />
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <p class="pintop">
                  I am looking for highly motivated graduate and undergraduate
                  students for research opportunities in machine learning,
                  computer vision, and generative models starting from Sep.
                  2025. Feel free to get in touch!
                </p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle" style="line-height: 0.8">
                <heading>News</heading>
                <p>
                  &#8226; [2025-01] I join the VCC group at Shenzhen University.
                </p>
                <p>
                  &#8226; [2024-12] I defend my PhD thesis at City University of
                  Hong Kong.
                </p>
                <p>
                  &#8226; [2024-11] I defend my PhD thesis at Nanjing
                  University.
                </p>
                <p>
                  &#8226; [2024-10] I am invited to give a talk at PRCV 2024 PhD
                  Workshop.
                </p>
                <p>
                  &#8226; [2024-06] One paper (LiesGAN) is accepted by PRCV
                  2024.
                </p>
                <p>
                  &#8226; [2024-03] One paper (Analogist) is accepted by
                  SIGGRAPH 2024.
                </p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <!-- color= #ef988c -->
                <heading> Publication </heading>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td style="width: 25%; vertical-align: middle">
                <img src="images/gu2024liesgan.jpg" class="teaser" />
              </td>
              <td valign="top" width="75%">
                <papertitle>
                  Task-Aware Few-Shot Image Generation via Dynamic Local
                  Distribution Estimation and Sampling
                </papertitle>
                <br />
                <strong>Zheng Gu</strong>, Wenbin Li*, Tianyu Ding, Zhengli
                Wang, Jing Huo, Kuihua Huang, and Yang Gao
                <br />
                <em
                  >Chinese Conference on Pattern Recognition and Computer Vision
                  (PRCV)</em
                >, 2024
                <br />
                <a
                  href="https://link.springer.com/chapter/10.1007/978-981-97-8490-5_33"
                  >[Paper]</a
                >
                <a>[Code]</a>
                <p></p>
              </td>
            </tr>

            <tr>
              <td style="width: 25%; vertical-align: middle">
                <img src="images/gu2024analogist.jpg" class="teaser" />
              </td>
              <td valign="top" width="75%">
                <papertitle>
                  Analogist: Out-of-the-box Visual In-Context Learning with
                  Image Diffusion Model
                </papertitle>
                <br />
                <strong>Zheng Gu</strong>, Shiyuan Yang, Jing Liao*, Jing Huo*,
                and Yang Gao
                <br />
                <em>ACM Transactions on Graphics (Proceedings of SIGGRAPH)</em>,
                2024
                <br />
                <a href="https://analogist2d.github.io/">[Project Page]</a>
                <a href="https://dl.acm.org/doi/10.1145/3658136">[Paper]</a>
                <a href="https://arxiv.org/abs/2405.10316">[Arxiv]</a>
                <a href="https://github.com/edward3862/Analogist">[Code]</a>
                <a href="https://huggingface.co/datasets/picana/Analogist"
                  >[Data]</a
                >
                <p></p>
              </td>
            </tr>

            <tr>
              <td style="width: 25%; vertical-align: middle">
                <img src="images/gu2021carime.png" class="teaser" />
              </td>
              <td valign="top" width="75%">
                <papertitle>
                  CariMe: Unpaired Caricature Generation with Multiple
                  Exaggerations
                </papertitle>
                <br />
                <strong>Zheng Gu</strong>, Chuanqi Dong, Jing Huo*, Wenbin Li,
                and Yang Gao
                <br />
                <em>IEEE Transactions on Multimedia (TMM)</em>, 2021
                <br />
                <a href="https://ieeexplore.ieee.org/abstract/document/9454341"
                  >[Paper]</a
                >
                <a href="https://arxiv.org/abs/2010.00246">[Arxiv]</a>
                <a href="https://github.com/edward3862/CariMe-pytorch"
                  >[Code]</a
                >
                <a href="https://cs.nju.edu.cn/rl/WebCaricature.htm">[Data]</a>
                <p></p>
              </td>
            </tr>

            <tr>
              <td style="width: 25%; vertical-align: middle">
                <img src="images/gu2021lofgan.png" class="teaser" />
              </td>
              <td valign="top" width="75%">
                <papertitle>
                  LoFGAN: Fusing Local Representations for Few-shot Image
                  Generation
                </papertitle>
                <br />
                <strong>Zheng Gu</strong
                ><span class="superscript">&#8224;</span>, Wenbin Li<span
                  class="superscript"
                  >&#8224;</span
                >, Jing Huo*, Lei Wang, and Yang Gao
                <br />
                <em>International Conference on Computer Vision (ICCV)</em>,
                2021
                <br />
                <a
                  href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gu_LoFGAN_Fusing_Local_Representations_for_Few-Shot_Image_Generation_ICCV_2021_paper.pdf"
                  >[Paper]</a
                >
                <a href="https://github.com/edward3862/LoFGAN-pytorch"
                  >[Code]</a
                >
                <a href="https://box.nju.edu.cn/d/27fe9433f56a481ebe70/"
                  >[Data]</a
                >
                <p></p>
              </td>
            </tr>

            <tr>
              <td style="width: 25%; vertical-align: middle">
                <img src="images/dong2020learning.png" class="teaser" />
              </td>
              <td valign="top" width="75%">
                <papertitle>
                  Learning Task-aware Local Representations for Few-shot
                  Learning
                </papertitle>
                <br />
                Chuanqi Dong, Wenbin Li, Jing Huo, <strong>Zheng Gu</strong>,
                and Yang Gao*
                <br />
                <em
                  >International Joint Conference on Artificial Intelligence
                  (IJCAI)</em
                >, 2020
                <br />
                <a href="https://www.ijcai.org/proceedings/2020/0100.pdf"
                  >[Paper]</a
                >
                <a href="https://github.com/LegenDong/ATL-Net">[Code]</a>
                <p></p>
              </td>
            </tr>

            <tr>
              <td style="width: 25%; vertical-align: middle">
                <img src="images/ji2020unsupervised.png" class="teaser" />
              </td>
              <td valign="top" width="75%">
                <papertitle>
                  Unsupervised Domain Attention Adaptation Network for
                  Caricature Attribute Recognition
                </papertitle>
                <br />
                Wen Ji, Kelei He, Jing Huo*, <strong>Zheng Gu</strong>, and Yang
                Gao
                <br />
                <em>European Conference on Computer Vision (ECCV)</em>, 2020
                <br />
                <a
                  href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123530018.pdf"
                  >[Paper]</a
                >
                <a href="https://github.com/KeleiHe/DAAN">[Code]</a>
                <p></p>
              </td>
            </tr>

            <tr>
              <td style="width: 25%; vertical-align: middle">
                <img src="images/dong2019deepmef.png" class="teaser" />
              </td>
              <td valign="top" width="75%">
                <papertitle>
                  DeepMEF: A Deep Model Ensemble Framework for Video Based
                  Multi-modal Person Identification
                </papertitle>
                <br />
                Chuanqi Dong, <strong>Zheng Gu</strong>, Zhonghao Huang, Wen Ji,
                Jing Huo, and Yang Gao
                <br />
                <em>ACM Conference on Multimedia (ACM MM)</em>, 2019
                <br />
                <a href="https://dl.acm.org/doi/abs/10.1145/3343031.3356057"
                  >[Paper]</a
                >
                <a href="https://github.com/LegenDong/IQIYI_VID_FACE_2019"
                  >[Code]</a
                >
                <p></p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Honors</heading>
                <p><b>Huawei Scholarship</b>, Nanjing University, 2021</p>
                <p><b>Suzhou Yucai Scholarship</b>, Nanjing University, 2021</p>
                <p>
                  <b>Outstanding Postgraduate Student</b>, Nanjing University,
                  2021
                </p>
                <p><b>PhD Talent Scholarship</b>, Nanjing University, 2020</p>
                <p>
                  <b>3rd</b> Place, iQIYI Celebrity Video Identification
                  Challenge of ACM MM , 2019
                </p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Teaching</heading>
                <p>
                  Web-based Programming, Shenzhen University, Instructor, 2025
                  Spring
                </p>
                <p>
                  CS3402: Database Systems, City University of HongKong,
                  Teaching Assistant (TA), 2022-2023
                </p>
                <p>
                  Object-oriented Design Method, Nanjing University, Teaching
                  Assistant (TA), 2019-2020
                </p>
                <p>
                  Artificial Intelligence, Nanjing University, Teaching
                  Assistant (TA), 2018-2019
                </p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Service</heading>
                <p>
                  <b>Conference/Journal Reviewer:</b> SIGGRAPH Asia, ICCV, ECCV,
                  ACCV, WACV, TVCG, Multimedia Systems.
                </p>
                <p>
                  <b>Executive Chair:</b> CCF Nanjing University Student Branch
                  in 2019-2021.
                </p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td width="100%" valign="middle">
                <heading>Experience</heading>
                <p>
                  <strong>2023.12~2024.07</strong> &nbsp &nbsp &nbsp Research
                  Assistant at CityU Shenzhen Research Institute
                </p>
                <p>
                  <strong>2017.08~2018.07</strong> &nbsp &nbsp &nbsp Serve as a
                  Volunteer Teacher at Yunnan Province, Southwest China
                </p>
              </td>
            </tr>
          </table>

          <table width="100%" align="center" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br />
                <p align="right">
                  You could steal the source code
                  <a href="https://github.com/jonbarron/jonbarron_website"
                    ><strong>here</strong></a
                  >
                </p>
              </td>
            </tr>
          </table>
          <script type="text/javascript">
            var gaJsHost =
              "https:" == document.location.protocol
                ? "https://ssl."
                : "http://www.";
            document.write(
              unescape(
                "%3Cscript src='" +
                  gaJsHost +
                  "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"
              )
            );
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-7580334-1");
              pageTracker._trackPageview();
            } catch (err) {}
          </script>
        </td>
      </tr>
    </table>
  </body>
</html>
